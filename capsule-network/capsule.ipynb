{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capsule1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zmQwP2-_Gl0A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.engine import data_adapter"
      ],
      "metadata": {
        "id": "RkpRtymlqAb3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FTzA6fy_Gvnp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def squash(v,epsilon=1e-7,axis=-1):\n",
        "    sqnrm=tf.reduce_sum(tf.square(v), axis=axis,keepdims=True)\n",
        "    nrm=tf.sqrt(sqnrm + epsilon) #safe norm to avoid divide by zero.\n",
        "    sqsh_factor = sqnrm / (1. + sqnrm)\n",
        "    unit_vect = v / nrm\n",
        "    return sqsh_factor*unit_vect\n",
        "\n",
        "@tf.function\n",
        "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False):\n",
        "        squared_norm = tf.reduce_sum(tf.square(s),axis=axis,keepdims=keep_dims)\n",
        "        return tf.sqrt(squared_norm + epsilon)"
      ],
      "metadata": {
        "id": "I3-j_MOkaCtf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading in appropriate formate\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
        "x_test = x_test[..., tf.newaxis].astype(\"float32\")\n",
        "\n",
        "y_train=tf.keras.utils.to_categorical(y_train)\n",
        "y_test=tf.keras.utils.to_categorical(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES7neHl9Gx-o",
        "outputId": "e8c3a50d-1b39-4b64-86b7-79fede99c52f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "1IaqLHuAwxYt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caps1_n_maps = 32\n",
        "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
        "caps1_n_dims = 8\n",
        "\n",
        "# digit capsule layer\n",
        "caps2_n_caps = 10 # 10 capsule each digit.\n",
        "caps2_n_dims = 16 # each of the 10 capsules are of 16 dims.\n"
      ],
      "metadata": {
        "id": "PL71lMZzG1Po"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Primary_caps_layer(tf.keras.layers.Layer):\n",
        "  \"\"\" caps_n(i) --> no of capsule in ith layer \n",
        "      caps_dim(i) --> dimension of capsule in ith layer. \n",
        "      \n",
        "      primary_caps_layer output shape = [batch_size,caps_n,caps_dim]\"\"\"\n",
        "\n",
        "  def __init__(self,caps_dim=8,caps_n=1152):\n",
        "    super(Primary_caps_layer, self).__init__()\n",
        "    self.caps_n=caps_n  # no of capsule in this layer.\n",
        "    self.caps_dim=caps_dim # dim of each capsule in this layer\n",
        "    self.conv1=tf.keras.layers.Conv2D(256,kernel_size=9,strides=1,padding='valid',activation='relu') #@ changes may be needed of no of kernel.\n",
        "    self.conv2=tf.keras.layers.Conv2D(256,kernel_size=9,strides=2,padding='valid',activation='relu')\n",
        "\n",
        "  def call(self, input_tensor):\n",
        "    x=self.conv1(input_tensor)\n",
        "    x=self.conv2(x)\n",
        "    x=tf.reshape(x,[-1,self.caps_n,self.caps_dim])\n",
        "    return squash(x)\n",
        "    "
      ],
      "metadata": {
        "id": "UJZ0c0hDKstw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Digit_caps_layer(tf.keras.layers.Layer):\n",
        "  \"\"\" caps_n(i) --> no of capsule in ith layer \n",
        "      caps_dim(i) --> dimension of capsule in ith layer. \n",
        "      and we assume this is ith layer. \n",
        "      output.shape of ith layer = [batch_size, 1,caps_n(i),caps_dim(i), 1]\"\"\"\n",
        "\n",
        "  def __init__(self,caps_dim=16,caps_n=10,r=3):\n",
        "    super(Digit_caps_layer,self).__init__()\n",
        "    self.caps_n=caps_n # no of capsule.\n",
        "    self.caps_dim=caps_dim # dim of each capsule.\n",
        "    self.r=r # no of iteration in routing by agreement algorithm.\n",
        "  \n",
        "  def build(self,input_shape): # input_shape = [batch_size,caps_n(i-1),caps_dim(i-1)] \n",
        "    self.W = tf.Variable(initial_value=tf.random.normal(\n",
        "    shape=(1, input_shape[1], self.caps_n, self.caps_dim, input_shape[-1]),\n",
        "    stddev=0.1, dtype=tf.float32),\n",
        "    trainable=True)  #weigth initialization for this layer W.shape=[1,caps_n(i-1),caps_n(i),caps_dim(i),caps_dim(i-1)].\n",
        "\n",
        "  def call(self,input_tensor): #input_tensor.shape=[batch_size,caps_n(i-1),caps_dim(i-1)]\n",
        "    batch_size = input_tensor.shape[0]\n",
        "    W_tiled = tf.tile(self.W, [batch_size, 1, 1, 1, 1]) # replicating the weights for parallel processing of a batch.\n",
        "    \"\"\" W_tiled.shape=[batch_size,caps_n(i-1),caps_n(i),caps_dim(i),caps_dim(i-1)] \"\"\"\n",
        "\n",
        "    caps_output_expanded = tf.expand_dims(input_tensor, -1) # converting last dim to a column vector.\n",
        "    \"\"\" the above step change the input shape from \n",
        "        [batch_size,caps_n(i-1),caps_dim(i-1)] --> [batch_size,caps_n(i-1),caps_dim(i-1),1]\"\"\"\n",
        "\n",
        "    caps_output_tile = tf.expand_dims(caps_output_expanded, 2)\n",
        "    \"\"\" the above step change the input shape from \n",
        "        [batch_size,caps_n(i-1),caps_dim(i-1),1] --> [batch_size,caps_n(i-1),1,caps_dim(i-1),1]\"\"\"\n",
        "\n",
        "    caps_output_tiled = tf.tile(caps_output_tile, [1, 1, self.caps_n, 1, 1]) # replicating the input capsule vector for every output capsule.\n",
        "    \" i.e [batch_size,caps_n(i-1),1,caps_dim(i-1),1] --> [batch_size,caps_n(i-1),caps_n(i),1,caps_dim(i-1),1]\"\n",
        "\n",
        "    caps_predicted = tf.matmul(W_tiled, caps_output_tiled) # this is performing element wise tf.matmul() operation.\n",
        "    \"\"\" caps_predicted.shape = [1,caps_n(i-1),caps_n(i),caps_dim(i),1]\"\"\"\n",
        "\n",
        "    \"\"\" dynamic routing \"\"\"\n",
        "    raw_weights = tf.zeros([batch_size,input_tensor.shape[1] , self.caps_n, 1, 1]) # non trainable weights.\n",
        "    \"\"\" raw_weights.shape=[batch_size,caps_n(i-1) ,caps_n(i), 1, 1]\"\"\"\n",
        "\n",
        "    r=self.r\n",
        "    while(r):\n",
        "      r-=1\n",
        "      routing_weights = tf.nn.softmax(raw_weights,axis=2)\n",
        "      \"\"\" [batch_size,caps_n(i-1) ,caps_n(i), 1, 1]  softmax applied along the pointed dim.\n",
        "                                       ^                                                   \"\"\"\n",
        "\n",
        "      weighted_predictions = tf.multiply(routing_weights, caps_predicted)\n",
        "      \"\"\" weighted_predictions.shape = [batch_size, caps_n(i-1),caps_n(i),caps_dim(i), 1]\"\"\"\n",
        "\n",
        "      weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keepdims=True)\n",
        "      \"\"\" [batch_size,caps_n(i-1) ,caps_n(i),caps_dim(i), 1]  sum applied along the pointed dim.\n",
        "                           ^                                                               \n",
        "      therefore weighted_sum.shape=[batch_size,1 ,caps_n(i),caps_dim(i), 1]\"\"\"\n",
        "\n",
        "      v = squash(weighted_sum, axis=-2) #normalize to unit length vector.\n",
        "      v_tiled = tf.tile(v, [1, input_tensor.shape[1], 1, 1, 1])\n",
        "      \"\"\" v_tiled.shape=[batch_size,caps_n(i-1),caps_n(i),caps_dim(i), 1]\"\"\"\n",
        "\n",
        "      agreement = tf.matmul(caps_predicted, v_tiled,transpose_a=True)\n",
        "      \"\"\" agreement.shape=[batch_size,caps_n(i-1),caps_n(i), 1, 1]\"\"\"\n",
        "\n",
        "      if(r>0):\n",
        "          routing_weights+=agreement\n",
        "      else:\n",
        "          return v"
      ],
      "metadata": {
        "id": "Upn8S10ai3M-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Caps_net(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,no_classes=10):\n",
        "    super(Caps_net,self).__init__()\n",
        "    self.pri_layer=Primary_caps_layer(caps_dim=8,caps_n=1152)\n",
        "    self.dig_layer=Digit_caps_layer(caps_dim=16,caps_n=10,r=3)\n",
        "\n",
        "  def call(self,input_tensor):\n",
        "    x = self.pri_layer(input_tensor) #x.shape=[batch_size,caps_n(i),caps_dim(i)]\n",
        "    x = self.dig_layer(x) #x.shape=[batch_size, 1,caps_n(i),caps_dim(i), 1]\n",
        "\n",
        "    \"\"\"The lengths of the output vectors represent the class probabilities, \n",
        "       so we could just use tf.norm() to compute them,\"\"\"\n",
        "    x = safe_norm(x, axis=-2) #x.shape=[batch_size,1,caps_n(i-1),1]\n",
        "\n",
        "    x = tf.nn.softmax(x,axis=2) #converting those probabilities to prob dist.\n",
        "    x = tf.squeeze(x, axis=[1,3]) #reducing the extra dims. therefore the output shape =[batch_size,caps_n(i-1)] \n",
        "    return x\n",
        "\n",
        "  \"\"\" custom training loop \n",
        "  def train_step(self,data):\n",
        "    x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n",
        "   \n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = self(x, training=True)  # Forward pass\n",
        "        # Compute the loss value\n",
        "        # (the loss function is configured in `compile()`)\n",
        "        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics} \"\"\"\n"
      ],
      "metadata": {
        "id": "Ehw2bjCPbCLF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Caps_net(no_classes=10)"
      ],
      "metadata": {
        "id": "LGqZX4_fhSS0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(x_train[:32]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvCka-C8hXda",
        "outputId": "fb4b8770-413c-4ff4-de87-9dacb42bb1fa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([32, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "          loss      = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "          metrics   = tf.keras.metrics.CategoricalAccuracy(),\n",
        "          optimizer = tf.keras.optimizers.Adam())"
      ],
      "metadata": {
        "id": "t3EVZ55DqnUf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=32,epochs=2,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2UPtw9WqbuO",
        "outputId": "e7af2499-19e7-46a9-a36d-474be4b3e481"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1500/1500 [==============================] - 75s 47ms/step - loss: 2.1907 - categorical_accuracy: 0.9607 - val_loss: 2.1799 - val_categorical_accuracy: 0.9842\n",
            "Epoch 2/2\n",
            "1500/1500 [==============================] - 73s 49ms/step - loss: 2.1784 - categorical_accuracy: 0.9861 - val_loss: 2.1775 - val_categorical_accuracy: 0.9879\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdcea5d8950>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k4Q-BchI86fR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}