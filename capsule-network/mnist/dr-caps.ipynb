{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "_kHqzGCNoGwp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def squash(x, axis=-1):\n",
        "    s_squared_norm = tf.math.reduce_sum(tf.math.square(x), axis, keepdims=True) + keras.backend.epsilon()\n",
        "    scale = tf.math.sqrt(s_squared_norm) / (1 + s_squared_norm)\n",
        "    return scale * x\n",
        "\n",
        "@tf.function\n",
        "def margin_loss(y_true, y_pred):\n",
        "    lamb, margin = 0.5, 0.1\n",
        "    return tf.math.reduce_sum((y_true * tf.math.square(tf.nn.relu(1 - margin - y_pred)) + lamb * (\n",
        "        1 - y_true) * tf.math.square(tf.nn.relu(y_pred - margin))), axis=-1)\n",
        "\n",
        "#@tf.function\n",
        "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False):\n",
        "        squared_norm = tf.reduce_sum(tf.square(s),axis=axis,keepdims=keep_dims)\n",
        "        return tf.sqrt(squared_norm + epsilon)"
      ],
      "metadata": {
        "id": "YOzOfUa8oGzG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Capsule(keras.layers.Layer):\n",
        "   \n",
        "    def __init__(self,\n",
        "                 num_capsule,\n",
        "                 dim_capsule,\n",
        "                 routings=3,\n",
        "                 **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.caps_n = num_capsule\n",
        "        self.caps_dim = dim_capsule\n",
        "        self.r = routings\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "        'num_capsule':  self.caps_n,\n",
        "        'dim_capsule' : self.caps_dim,\n",
        "        'routings':  self.r,      \n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \n",
        "        self.W = self.add_weight(name='W',\n",
        "                    shape=[1, input_shape[1], self.caps_n, self.caps_dim, input_shape[-1]],\n",
        "                    dtype=tf.float32,\n",
        "                    initializer='glorot_uniform',\n",
        "                    trainable=True)\n",
        "\n",
        "    def call(self, input_tensor):\n",
        "        batch_size = input_tensor.shape[0]\n",
        "        \n",
        "        W_tiled = tf.tile(self.W, [batch_size, 1, 1, 1, 1]) # replicating the weights for parallel processing of a batch.\n",
        "        \"\"\" W_tiled.shape=[batch_size,caps_n(i-1),caps_n(i),caps_dim(i),caps_dim(i-1)] \"\"\"\n",
        "\n",
        "        caps_output_expanded = tf.expand_dims(input_tensor, -1) # converting last dim to a column vector.\n",
        "        \"\"\" the above step change the input shape from \n",
        "            [batch_size,caps_n(i-1),caps_dim(i-1)] --> [batch_size,caps_n(i-1),caps_dim(i-1),1]\"\"\"\n",
        "\n",
        "        caps_output_tile = tf.expand_dims(caps_output_expanded, 2)\n",
        "        \"\"\" the above step change the input shape from \n",
        "            [batch_size,caps_n(i-1),caps_dim(i-1),1] --> [batch_size,caps_n(i-1),1,caps_dim(i-1),1]\"\"\"\n",
        "\n",
        "        caps_output_tiled = tf.tile(caps_output_tile, [1, 1, self.caps_n, 1, 1]) # replicating the input capsule vector for every output capsule.\n",
        "        \"\"\" i.e [batch_size,caps_n(i-1),1,caps_dim(i-1),1] --> [batch_size,caps_n(i-1),caps_n(i),1,caps_dim(i-1),1]\"\"\"\n",
        "\n",
        "        caps_predicted = tf.matmul(W_tiled, caps_output_tiled) # this is performing element wise tf.matmul() operation.\n",
        "        \"\"\" caps_predicted.shape = [1,caps_n(i-1),caps_n(i),caps_dim(i),1]\"\"\"\n",
        "\n",
        "        \"\"\" dynamic routing \"\"\"\n",
        "        raw_weights = tf.zeros([batch_size,input_tensor.shape[1] , self.caps_n, 1, 1]) # non trainable weights.\n",
        "        \"\"\" raw_weights.shape=[batch_size,caps_n(i-1) ,caps_n(i), 1, 1]\"\"\"\n",
        "\n",
        "        r=self.r\n",
        "        while(r):\n",
        "          r-=1\n",
        "          routing_weights = tf.nn.softmax(raw_weights,axis=2)\n",
        "          \"\"\" [batch_size,caps_n(i-1) ,caps_n(i), 1, 1]  softmax applied along the pointed dim.\n",
        "                                           ^                                                   \"\"\"\n",
        "\n",
        "          weighted_predictions = tf.multiply(routing_weights, caps_predicted)\n",
        "          \"\"\" weighted_predictions.shape = [batch_size, caps_n(i-1),caps_n(i),caps_dim(i), 1]\"\"\"\n",
        "\n",
        "          weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keepdims=True)\n",
        "          \"\"\" [batch_size,caps_n(i-1) ,caps_n(i),caps_dim(i), 1]  sum applied along the pointed dim.\n",
        "                               ^                                                               \n",
        "          therefore weighted_sum.shape=[batch_size,1 ,caps_n(i),caps_dim(i), 1]\"\"\"\n",
        "\n",
        "          v = squash(weighted_sum, axis=-2) #normalize to unit length vector.\n",
        "          v_tiled = tf.tile(v, [1, input_tensor.shape[1], 1, 1, 1])\n",
        "          \"\"\" v_tiled.shape=[batch_size,caps_n(i-1),caps_n(i),caps_dim(i), 1]\"\"\"\n",
        "\n",
        "          agreement = tf.matmul(caps_predicted, v_tiled,transpose_a=True)\n",
        "          \"\"\" agreement.shape=[batch_size,caps_n(i-1),caps_n(i), 1, 1]\"\"\"\n",
        "\n",
        "          if(r>0):\n",
        "              routing_weights+=agreement\n",
        "          else:\n",
        "              v = tf.squeeze(v, axis=[1,4])\n",
        "              return v\n",
        "\n",
        "    def compute_output_signature(self,input_shape):\n",
        "      return tf.TensorSpec(shape=[input_shape[0],self.caps_n,self.caps_dim],dtype=tf.float32) "
      ],
      "metadata": {
        "id": "5-T8sFDEoG2E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c1=tf.keras.layers.Conv2D(16,kernel_size=5,strides=1,padding='valid',activation='relu')\n",
        "c2=tf.keras.layers.Conv2D(32,kernel_size=9,strides=1,padding='valid',activation='relu')\n",
        "bn1=tf.keras.layers.BatchNormalization()\n",
        "bn2=tf.keras.layers.BatchNormalization()\n",
        "last=Capsule(10,16)"
      ],
      "metadata": {
        "id": "Eqj28O3EoG_K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = keras.Input(shape=(28,28,1), batch_size=32)\n",
        "x=c1(model_input)\n",
        "x=bn1(x,training=True)\n",
        "x=c2(x)\n",
        "x=bn2(x,training=True)\n",
        "x=tf.reshape(x,[-1,16*32,16])\n",
        "x=last(x)\n",
        "#x=tf.cast(x,tf.float32)\n",
        "x=safe_norm(x, axis=2)\n",
        "model_output = x\n"
      ],
      "metadata": {
        "id": "esnIdI8-oHCZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Model(model_input, model_output)"
      ],
      "metadata": {
        "id": "-U6EXeQWoHNC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0001) \n",
        "model.compile(loss=margin_loss,\n",
        "              optimizer=adam,\n",
        "              metrics=tf.keras.metrics.CategoricalAccuracy())\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1OpHhb2oHPW",
        "outputId": "dfd36958-d2cd-47ac-de68-12cbb1ccf6a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(32, 28, 28, 1)]         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (32, 24, 24, 16)          416       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (32, 24, 24, 16)         64        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (32, 16, 16, 32)          41504     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (32, 16, 16, 32)         128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " tf.reshape (TFOpLambda)     (32, 512, 16)             0         \n",
            "                                                                 \n",
            " capsule (Capsule)           (32, 10, 16)              1310720   \n",
            "                                                                 \n",
            " tf.math.square (TFOpLambda)  (32, 10, 16)             0         \n",
            "                                                                 \n",
            " tf.math.reduce_sum (TFOpLam  (32, 10)                 0         \n",
            " bda)                                                            \n",
            "                                                                 \n",
            " tf.__operators__.add (TFOpL  (32, 10)                 0         \n",
            " ambda)                                                          \n",
            "                                                                 \n",
            " tf.math.sqrt (TFOpLambda)   (32, 10)                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,352,832\n",
            "Trainable params: 1,352,736\n",
            "Non-trainable params: 96\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading in appropriate formate\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
        "x_test = x_test[..., tf.newaxis].astype(\"float32\")\n",
        "\n",
        "y_train=tf.keras.utils.to_categorical(y_train)\n",
        "y_test=tf.keras.utils.to_categorical(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1x7HEYzoHRs",
        "outputId": "19e692cd-0dc2-459b-9167-258d0f888f35"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=32,epochs=5,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBLjdeUZoHYm",
        "outputId": "2be6adb6-a35a-41a7-b5fe-25efa0e6a7b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 28s 12ms/step - loss: 0.0476 - categorical_accuracy: 0.9677 - val_loss: 0.0231 - val_categorical_accuracy: 0.9862\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 17s 12ms/step - loss: 0.0191 - categorical_accuracy: 0.9897 - val_loss: 0.0183 - val_categorical_accuracy: 0.9895\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 0.0139 - categorical_accuracy: 0.9937 - val_loss: 0.0166 - val_categorical_accuracy: 0.9906\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0113 - categorical_accuracy: 0.9954 - val_loss: 0.0145 - val_categorical_accuracy: 0.9912\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 0.0095 - categorical_accuracy: 0.9966 - val_loss: 0.0140 - val_categorical_accuracy: 0.9916\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faacc664ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  "
      ],
      "metadata": {
        "id": "0x3OPyZAoHbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}