{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmQwP2-_Gl0A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.engine import data_adapter"
      ],
      "metadata": {
        "id": "RkpRtymlqAb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FTzA6fy_Gvnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "njM193-baa26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def squash(v,epsilon=1e-7,axis=-1):\n",
        "    sqnrm=tf.reduce_sum(tf.square(v), axis=axis,keepdims=True)\n",
        "    nrm=tf.sqrt(sqnrm + epsilon) #safe norm to avoid divide by zero.\n",
        "    sqsh_factor = sqnrm / (1. + sqnrm)\n",
        "    unit_vect = v / nrm\n",
        "    return sqsh_factor*unit_vect\n",
        "\n",
        "@tf.function\n",
        "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False):\n",
        "        squared_norm = tf.reduce_sum(tf.square(s),axis=axis,keepdims=keep_dims)\n",
        "        return tf.sqrt(squared_norm + epsilon)"
      ],
      "metadata": {
        "id": "I3-j_MOkaCtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#downloading data.\n",
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
        "\n",
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (160, 160)\n",
        "\n",
        "#train data\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
        "                                                            shuffle=True,\n",
        "                                                            batch_size=BATCH_SIZE,\n",
        "                                                            image_size=IMG_SIZE)\n",
        "\n",
        "print('Number of validation batches: %d' % tf.data.experimental.cardinality(train_dataset))\n",
        "\n",
        "#validation model.\n",
        "validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
        "                                                                 shuffle=True,\n",
        "                                                                 batch_size=BATCH_SIZE,\n",
        "                                                                 image_size=IMG_SIZE)\n",
        "\n",
        "\n",
        "# creating test data.\n",
        "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
        "test_dataset = validation_dataset.take(val_batches // 5)\n",
        "validation_dataset = validation_dataset.skip(val_batches // 5)\n",
        "\n",
        "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
        "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))\n",
        "\n",
        "\n",
        "#y_train=tf.keras.utils.to_categorical(y_train)\n",
        "#y_test=tf.keras.utils.to_categorical(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES7neHl9Gx-o",
        "outputId": "73202693-40c7-48f4-d1c9-850e46b4585a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "68608000/68606236 [==============================] - 0s 0us/step\n",
            "68616192/68606236 [==============================] - 0s 0us/step\n",
            "Found 2000 files belonging to 2 classes.\n",
            "Number of validation batches: 63\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Number of validation batches: 26\n",
            "Number of test batches: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#optimization parameter setting.\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "1IaqLHuAwxYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Primary_caps_layer(tf.keras.layers.Layer):\n",
        "  \"\"\" caps_n(i) --> no of capsule in ith layer \n",
        "      caps_dim(i) --> dimension of capsule in ith layer. \n",
        "      \n",
        "      primary_caps_layer output shape = [batch_size,caps_n,caps_dim]\"\"\"\n",
        "\n",
        "  def __init__(self,caps_n=1152,k1=256,k2=256,k_s1=9,k_s2=5,s1=1,s2=3):\n",
        "    super(Primary_caps_layer, self).__init__()\n",
        "    self.caps_n=caps_n  # no of capsule in this layer.(as initialized by usr this may be changed based on other parameters.)\n",
        "    self.k1=k1          # no of filter in 1st conv layer.\n",
        "    self.k2=k2          # no of filter in 2nd conv layer.\n",
        "    self.k_s1=k_s1      # kernel_size of 1st conv layer.\n",
        "    self.k_s2=k_s2      # kernel_size of 2nd conv layer.\n",
        "    self.s1=s1          # stride in 1st conv layer.\n",
        "    self.s2=s2          # stride in 2nd conv layer.\n",
        "    self.conv1=tf.keras.layers.Conv2D(k1,kernel_size=k_s1,strides=s1,padding='valid',activation='relu') \n",
        "    self.conv2=tf.keras.layers.Conv2D(k2,kernel_size=k_s2,strides=s2,padding='valid',activation='relu')\n",
        "\n",
        "  def call(self, input_tensor):\n",
        "    batch_size=input_tensor.shape[0]\n",
        "    x=self.conv1(input_tensor)\n",
        "    x=self.conv2(x) \n",
        "\n",
        "    assert x.shape[1]*x.shape[1]*self.k2==self.caps_n*self.caps_dim # $ eqn--1\n",
        "\n",
        "    x=tf.reshape(x,[batch_size,self.caps_n,self.caps_dim]) # *\n",
        "    return squash(x)\n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.batch_size=input_shape[0] \n",
        "    tmp=int(((input_shape[1]-self.k_s1)/self.s1))+1\n",
        "    self.conv1_output_shape=[input_shape[0],tmp,tmp,self.k1]\n",
        "    tmp=int(((tmp-self.k_s2)/self.s2))+1\n",
        "    self.conv2_output_shape=[input_shape[0],tmp,tmp,self.k2]\n",
        "    tmp1=tmp*tmp*self.k2\n",
        "    self.caps_n=self.caps_n-(tmp1%self.caps_n) # recomputing apropriate no of capsule : $ eqn--1 is true.\n",
        "    self.caps_dim=int((tmp*tmp*self.k2)/self.caps_n); # same is done for caps_dim.\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "UJZ0c0hDKstw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Digit_caps_layer(tf.keras.layers.Layer):\n",
        "  \"\"\" caps_n(i) --> no of capsule in ith layer \n",
        "      caps_dim(i) --> dimension of capsule in ith layer. \n",
        "      and we assume this is ith layer. \n",
        "      output.shape of ith layer = [batch_size, 1,caps_n(i),caps_dim(i), 1]\"\"\"\n",
        "\n",
        "  def __init__(self,caps_dim=16,caps_n=10,r=3):\n",
        "    super(Digit_caps_layer,self).__init__()\n",
        "    self.caps_n=caps_n # no of capsule.\n",
        "    self.caps_dim=caps_dim # dim of each capsule.\n",
        "    self.r=r # no of iteration in routing by agreement algorithm.\n",
        "    \n",
        "  def build(self,input_shape): # input_shape = [batch_size,caps_n(i-1),caps_dim(i-1)] \n",
        "    self.W = tf.Variable(initial_value=tf.random.normal(\n",
        "    shape=(1, input_shape[1], self.caps_n, self.caps_dim, input_shape[-1]),\n",
        "    stddev=0.1, dtype=tf.float32),\n",
        "    trainable=True)  #weigth initialization for this layer W.shape=[1,caps_n(i-1),caps_n(i),caps_dim(i),caps_dim(i-1)].\n",
        "\n",
        "  def call(self,input_tensor): #input_tensor.shape=[batch_size,caps_n(i-1),caps_dim(i-1)]\n",
        "    batch_size = input_tensor.shape[0]\n",
        "    W_tiled = tf.tile(self.W, [batch_size, 1, 1, 1, 1]) # replicating the weights for parallel processing of a batch.\n",
        "    \"\"\" W_tiled.shape=[batch_size,caps_n(i-1),caps_n(i),caps_dim(i),caps_dim(i-1)] \"\"\"\n",
        "\n",
        "    caps_output_expanded = tf.expand_dims(input_tensor, -1) # converting last dim to a column vector.\n",
        "    \"\"\" the above step change the input shape from \n",
        "        [batch_size,caps_n(i-1),caps_dim(i-1)] --> [batch_size,caps_n(i-1),caps_dim(i-1),1]\"\"\"\n",
        "\n",
        "    caps_output_tile = tf.expand_dims(caps_output_expanded, 2)\n",
        "    \"\"\" the above step change the input shape from \n",
        "        [batch_size,caps_n(i-1),caps_dim(i-1),1] --> [batch_size,caps_n(i-1),1,caps_dim(i-1),1]\"\"\"\n",
        "\n",
        "    caps_output_tiled = tf.tile(caps_output_tile, [1, 1, self.caps_n, 1, 1]) # replicating the input capsule vector for every output capsule.\n",
        "    \"\"\" i.e [batch_size,caps_n(i-1),1,caps_dim(i-1),1] --> [batch_size,caps_n(i-1),caps_n(i),1,caps_dim(i-1),1]\"\"\"\n",
        "\n",
        "    caps_predicted = tf.matmul(W_tiled, caps_output_tiled) # this is performing element wise tf.matmul() operation.\n",
        "    \"\"\" caps_predicted.shape = [1,caps_n(i-1),caps_n(i),caps_dim(i),1]\"\"\"\n",
        "\n",
        "    \"\"\" dynamic routing \"\"\"\n",
        "    raw_weights = tf.zeros([batch_size,input_tensor.shape[1] , self.caps_n, 1, 1]) # non trainable weights.\n",
        "    \"\"\" raw_weights.shape=[batch_size,caps_n(i-1) ,caps_n(i), 1, 1]\"\"\"\n",
        "\n",
        "    r=self.r\n",
        "    while(r):\n",
        "      r-=1\n",
        "      routing_weights = tf.nn.softmax(raw_weights,axis=2)\n",
        "      \"\"\" [batch_size,caps_n(i-1) ,caps_n(i), 1, 1]  softmax applied along the pointed dim.\n",
        "                                       ^                                                   \"\"\"\n",
        "\n",
        "      weighted_predictions = tf.multiply(routing_weights, caps_predicted)\n",
        "      \"\"\" weighted_predictions.shape = [batch_size, caps_n(i-1),caps_n(i),caps_dim(i), 1]\"\"\"\n",
        "\n",
        "      weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keepdims=True)\n",
        "      \"\"\" [batch_size,caps_n(i-1) ,caps_n(i),caps_dim(i), 1]  sum applied along the pointed dim.\n",
        "                           ^                                                               \n",
        "      therefore weighted_sum.shape=[batch_size,1 ,caps_n(i),caps_dim(i), 1]\"\"\"\n",
        "\n",
        "      v = squash(weighted_sum, axis=-2) #normalize to unit length vector.\n",
        "      v_tiled = tf.tile(v, [1, input_tensor.shape[1], 1, 1, 1])\n",
        "      \"\"\" v_tiled.shape=[batch_size,caps_n(i-1),caps_n(i),caps_dim(i), 1]\"\"\"\n",
        "\n",
        "      agreement = tf.matmul(caps_predicted, v_tiled,transpose_a=True)\n",
        "      \"\"\" agreement.shape=[batch_size,caps_n(i-1),caps_n(i), 1, 1]\"\"\"\n",
        "\n",
        "      if(r>0):\n",
        "          routing_weights+=agreement\n",
        "      else:\n",
        "          return v"
      ],
      "metadata": {
        "id": "Upn8S10ai3M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Caps_net(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,no_classes=10):\n",
        "    super(Caps_net,self).__init__()\n",
        "    self.no_classes=no_classes\n",
        "    self.pri_layer=Primary_caps_layer(caps_n=256,k1=64,k2=64,k_s1=9,k_s2=5,s1=1,s2=3)\n",
        "    self.dig_layer=Digit_caps_layer(caps_dim=8,caps_n=no_classes,r=3)\n",
        "\n",
        "    self.decoder=tf.keras.Sequential([\n",
        "      keras.layers.Dense(128, activation='relu'),\n",
        "      keras.layers.Dense(128, activation='relu'),\n",
        "      keras.layers.Dense(160*160*3, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "  def call(self,input_tensor,y,training=False):\n",
        "    \"\"\" y should not be prob. dist/one-hot vectors it should be list of label for mnist it would \n",
        "        be as [1,4,6,3,8,7,...,5]. \n",
        "        when training is false y is not needed.\"\"\"\n",
        "\n",
        "    batch_size=input_tensor.shape[0]\n",
        "    img_dim=input_tensor.shape[1] # considering image size=(img_dim,img_dim,img_depth)\n",
        "    img_depth=input_tensor.shape[-1]\n",
        "\n",
        "    x = self.pri_layer(input_tensor) #x.shape=[batch_size,caps_n(i),caps_dim(i)]\n",
        "    x = self.dig_layer(x) #x.shape=[batch_size, 1,caps_n(i),caps_dim(i), 1]\n",
        "    z = safe_norm(x, axis=-2) #x.shape=[batch_size,1,caps_n(i-1),1]\n",
        "    z = tf.nn.softmax(z,axis=2) #converting those probabilities to prob dist.\n",
        "    y_pred = tf.squeeze(z, axis=[1,3]) #reducing the extra dims. therefore the output shape =[batch_size,caps_n(i-1)] \n",
        "    if(training==False):\n",
        "      return y_pred  # y_pred is a prob. dist.\n",
        "\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False)(tf.one_hot(y,depth=self.no_classes), y_pred)\n",
        "\n",
        "    #loss2 i.e reconstruction loss.\n",
        "    reconstruction_mask = tf.one_hot(y,depth=self.no_classes) # recon_mask is one-hot vect rep. of y.\n",
        "    \n",
        "    reconstruction_mask_reshaped = tf.reshape(reconstruction_mask, [batch_size, 1, self.no_classes, 1, 1])\n",
        "    # above reshape is done so that we can apply the mask.\n",
        "    lastcaps_output_masked = tf.multiply(x, reconstruction_mask_reshaped)\n",
        "\n",
        "    lastcaps_n=x.shape[2] # no of capsule in last layer.\n",
        "    lastcaps_dims=x.shape[3] # dim of capsule in last layer.\n",
        "\n",
        "    decoder_input = tf.reshape(lastcaps_output_masked,[batch_size, lastcaps_n * lastcaps_dims])\n",
        "    \n",
        "    decoder_output=self.decoder(decoder_input) \n",
        "    \"\"\" reconstruction of the input image based on the output vector of last layer\n",
        "        we apply the mask to the output of the last layer such that only the vector corresponding to a\n",
        "        particular lable is passed to the decoder.\"\"\"\n",
        "\n",
        "    X_flat = tf.reshape(input_tensor, [batch_size,img_dim*img_dim*img_depth]) # $ changes may be needed.\n",
        "    \n",
        "    squared_difference = tf.square(X_flat - decoder_output)\n",
        "    reconstruction_loss = tf.reduce_mean(squared_difference) # computation of mean squared loss between input image and reconstructed image.\n",
        "  \n",
        "    return loss+0.0005*reconstruction_loss\n",
        "    \n",
        "  \n"
      ],
      "metadata": {
        "id": "tEn4tRgT4Ww-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Caps_net(no_classes=2)"
      ],
      "metadata": {
        "id": "yJz60-4JpgBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('./checkpoints/my_checkpoint')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnFPuqf9Xsli",
        "outputId": "a4528ac9-4937-4361-f179-9d5c7d9f512e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd62af6d0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"customize training loop.\"\"\"\n",
        "\n",
        "# Instantiate an optimizer to train the model.\n",
        "base_learning_rate = 0.0001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "metadata": {
        "id": "6F_P1WSFqT2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nepoch {}/{}\".format(epoch+1,epochs))\n",
        "    pbar = keras.utils.Progbar(target=int(train_dataset.cardinality()))\n",
        "    metrics = {}\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred=model(x_batch_train,y_batch_train,training=False) # $ better design needed.\n",
        "            # y_pred is prob. dist.\n",
        "            loss_value = model(x_batch_train,y_batch_train,training=True) # loss computation\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights) # back prop\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) # weight update\n",
        "\n",
        "        # Update training metric.\n",
        "        train_acc_metric.update_state(tf.keras.utils.to_categorical(y_batch_train,num_classes=2), y_pred)\n",
        "        metrics.update({'train_acc':train_acc_metric.result()})\n",
        "        pbar.update(step+1, values=metrics.items(), finalize=False)\n",
        "\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in validation_dataset:\n",
        "      y_batch_val=tf.keras.utils.to_categorical(y_batch_val,num_classes=2)\n",
        "      val_pred = model(x_batch_val,y_batch_val,training=False) # $ better design needed\n",
        "      # Update val metrics\n",
        "      val_acc_metric.update_state(y_batch_val, val_pred)\n",
        "\n",
        "    metrics.update({'val_acc':val_acc_metric.result()})\n",
        "    \n",
        "    pbar.update(step+1, values=metrics.items(), finalize=True)\n",
        "    \n",
        "    # Reset training & val metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "    val_acc_metric.reset_states()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmp_nTBM5L9t",
        "outputId": "dacffe5d-b87f-4f08-e991-2ac482099e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 1/30\n",
            "63/63 [==============================] - 10s 157ms/step - train_acc: 0.9082 - val_acc: 0.6597\n",
            "\n",
            "epoch 2/30\n",
            "63/63 [==============================] - 10s 155ms/step - train_acc: 0.9232 - val_acc: 0.6609\n",
            "\n",
            "epoch 3/30\n",
            "63/63 [==============================] - 10s 159ms/step - train_acc: 0.9414 - val_acc: 0.6696\n",
            "\n",
            "epoch 4/30\n",
            "63/63 [==============================] - 10s 157ms/step - train_acc: 0.9548 - val_acc: 0.6510\n",
            "\n",
            "epoch 5/30\n",
            "63/63 [==============================] - 10s 157ms/step - train_acc: 0.9539 - val_acc: 0.6609\n",
            "\n",
            "epoch 6/30\n",
            "63/63 [==============================] - 12s 195ms/step - train_acc: 0.9674 - val_acc: 0.6609\n",
            "\n",
            "epoch 7/30\n",
            "63/63 [==============================] - 10s 157ms/step - train_acc: 0.9738 - val_acc: 0.6559\n",
            "\n",
            "epoch 8/30\n",
            "63/63 [==============================] - 10s 158ms/step - train_acc: 0.9746 - val_acc: 0.6634\n",
            "\n",
            "epoch 9/30\n",
            "63/63 [==============================] - 12s 181ms/step - train_acc: 0.9760 - val_acc: 0.6671\n",
            "\n",
            "epoch 10/30\n",
            "63/63 [==============================] - 12s 181ms/step - train_acc: 0.9796 - val_acc: 0.6745\n",
            "\n",
            "epoch 11/30\n",
            "63/63 [==============================] - 10s 158ms/step - train_acc: 0.9840 - val_acc: 0.6547\n",
            "\n",
            "epoch 12/30\n",
            "63/63 [==============================] - 10s 159ms/step - train_acc: 0.9851 - val_acc: 0.6584\n",
            "\n",
            "epoch 13/30\n",
            "63/63 [==============================] - 10s 158ms/step - train_acc: 0.9830 - val_acc: 0.6696\n",
            "\n",
            "epoch 14/30\n",
            "63/63 [==============================] - 10s 158ms/step - train_acc: 0.9910 - val_acc: 0.6522\n",
            "\n",
            "epoch 15/30\n",
            "63/63 [==============================] - 10s 159ms/step - train_acc: 0.9892 - val_acc: 0.6646\n",
            "\n",
            "epoch 16/30\n",
            "63/63 [==============================] - 10s 160ms/step - train_acc: 0.9922 - val_acc: 0.6547\n",
            "\n",
            "epoch 17/30\n",
            "63/63 [==============================] - 10s 158ms/step - train_acc: 0.9920 - val_acc: 0.6584\n",
            "\n",
            "epoch 18/30\n",
            "63/63 [==============================] - 10s 158ms/step - train_acc: 0.9916 - val_acc: 0.6572\n",
            "\n",
            "epoch 19/30\n",
            "63/63 [==============================] - 12s 181ms/step - train_acc: 0.9906 - val_acc: 0.6683\n",
            "\n",
            "epoch 20/30\n",
            "63/63 [==============================] - 12s 181ms/step - train_acc: 0.9902 - val_acc: 0.6621\n",
            "\n",
            "epoch 21/30\n",
            "63/63 [==============================] - 12s 181ms/step - train_acc: 0.9943 - val_acc: 0.6597\n",
            "\n",
            "epoch 22/30\n",
            "63/63 [==============================] - 12s 182ms/step - train_acc: 0.9928 - val_acc: 0.6559\n",
            "\n",
            "epoch 23/30\n",
            "63/63 [==============================] - 12s 181ms/step - train_acc: 0.9939 - val_acc: 0.6646\n",
            "\n",
            "epoch 24/30\n",
            "63/63 [==============================] - 12s 181ms/step - train_acc: 0.9950 - val_acc: 0.6646\n",
            "\n",
            "epoch 25/30\n",
            "63/63 [==============================] - 12s 181ms/step - train_acc: 0.9957 - val_acc: 0.6535\n",
            "\n",
            "epoch 26/30\n",
            "63/63 [==============================] - 12s 181ms/step - train_acc: 0.9955 - val_acc: 0.6671\n",
            "\n",
            "epoch 27/30\n",
            "63/63 [==============================] - 12s 181ms/step - train_acc: 0.9966 - val_acc: 0.6572\n",
            "\n",
            "epoch 28/30\n",
            "63/63 [==============================] - 10s 159ms/step - train_acc: 0.9961 - val_acc: 0.6634\n",
            "\n",
            "epoch 29/30\n",
            "63/63 [==============================] - 12s 181ms/step - train_acc: 0.9977 - val_acc: 0.6646\n",
            "\n",
            "epoch 30/30\n",
            "63/63 [==============================] - 12s 181ms/step - train_acc: 0.9981 - val_acc: 0.6522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiIeSpgf8o4K",
        "outputId": "cb3c5d67-c234-438e-aa00-90e5dfeab2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint  my_checkpoint.data-00000-of-00001  my_checkpoint.index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the weights\n",
        "model.save_weights('./checkpoints/my_checkpoint')"
      ],
      "metadata": {
        "id": "PdKdt2DHTbG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d6nHMX9S85t3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}