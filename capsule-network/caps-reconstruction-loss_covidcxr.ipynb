{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RRJZsHqg_qL3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from caps_layer import *"
      ],
      "metadata": {
        "id": "uCCOKdqPRM_q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Caps_net(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,no_classes=10):\n",
        "    super(Caps_net,self).__init__()\n",
        "    self.no_classes=no_classes\n",
        "\n",
        "    self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    self.loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "    self.train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "    self.val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    self.pri_layer=Primary_caps_layer(caps_n=256,k1=64,k2=64,k_s1=9,k_s2=5,s1=1,s2=3)\n",
        "    self.dig_layer=Digit_caps_layer(caps_dim=8,caps_n=no_classes,r=3)\n",
        "\n",
        "    self.decoder=tf.keras.Sequential([\n",
        "      keras.layers.Dense(128, activation='relu'),\n",
        "      keras.layers.Dense(128, activation='relu'),\n",
        "      keras.layers.Dense(256*256*3, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "  def call(self,input_tensor,y,training=False):\n",
        "    \"\"\" y should not be prob. dist/one-hot vectors it should be list of label for mnist it would \n",
        "        be as [1,4,6,3,8,7,...,5]. \n",
        "        when training is false y is not needed.\"\"\"\n",
        "\n",
        "    batch_size=input_tensor.shape[0]\n",
        "    img_dim=input_tensor.shape[1] # considering image size=(img_dim,img_dim,img_depth)\n",
        "    img_depth=input_tensor.shape[-1]\n",
        "\n",
        "    x = self.pri_layer(input_tensor) #x.shape=[batch_size,caps_n(i),caps_dim(i)]\n",
        "    x = self.dig_layer(x) #x.shape=[batch_size, 1,caps_n(i),caps_dim(i), 1]\n",
        "    z = safe_norm(x, axis=-2) #x.shape=[batch_size,1,caps_n(i-1),1]\n",
        "    z = tf.nn.softmax(z,axis=2) #converting those probabilities to prob dist.\n",
        "    y_pred = tf.squeeze(z, axis=[1,3]) #reducing the extra dims. therefore the output shape =[batch_size,caps_n(i-1)] \n",
        "    if(training==False):\n",
        "      return y_pred  # y_pred is a prob. dist.\n",
        "\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False)(tf.one_hot(y,depth=self.no_classes), y_pred)\n",
        "\n",
        "    #loss2 i.e reconstruction loss.\n",
        "    reconstruction_mask = tf.one_hot(y,depth=self.no_classes) # recon_mask is one-hot vect rep. of y.\n",
        "    \n",
        "    reconstruction_mask_reshaped = tf.reshape(reconstruction_mask, [batch_size, 1, self.no_classes, 1, 1])\n",
        "    # above reshape is done so that we can apply the mask.\n",
        "    lastcaps_output_masked = tf.multiply(x, reconstruction_mask_reshaped)\n",
        "\n",
        "    lastcaps_n=x.shape[2] # no of capsule in last layer.\n",
        "    lastcaps_dims=x.shape[3] # dim of capsule in last layer.\n",
        "\n",
        "    decoder_input = tf.reshape(lastcaps_output_masked,[batch_size, lastcaps_n * lastcaps_dims])\n",
        "    \n",
        "    decoder_output=self.decoder(decoder_input) \n",
        "    \"\"\" reconstruction of the input image based on the output vector of last layer\n",
        "        we apply the mask to the output of the last layer such that only the vector corresponding to a\n",
        "        particular lable is passed to the decoder.\"\"\"\n",
        "\n",
        "    X_flat = tf.reshape(input_tensor, [batch_size,img_dim*img_dim*img_depth]) \n",
        "    \n",
        "    squared_difference = tf.square(X_flat - decoder_output)\n",
        "    reconstruction_loss = tf.reduce_mean(squared_difference) # computation of mean squared loss between input image and reconstructed image.\n",
        "  \n",
        "    return loss+0.0005*reconstruction_loss\n",
        "\n",
        "  def fit(self,train_dataset,validation_dataset,epochs=3):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      print(\"\\nepoch {}/{}\".format(epoch+1,epochs))\n",
        "      pbar = keras.utils.Progbar(target=int(train_dataset.cardinality()))\n",
        "      metrics = {}\n",
        "\n",
        "      # Iterate over the batches of the dataset.\n",
        "      for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "          with tf.GradientTape() as tape:\n",
        "              y_pred=self(x_batch_train,y_batch_train,training=False) # $ better design needed.\n",
        "              # y_pred is prob. dist.\n",
        "              loss_value = self(x_batch_train,y_batch_train,training=True) # loss computation\n",
        "          grads = tape.gradient(loss_value, self.trainable_weights) # back prop\n",
        "          self.optimizer.apply_gradients(zip(grads, self.trainable_weights)) # weight update\n",
        "\n",
        "          # Update training metric.\n",
        "          self.train_acc_metric.update_state(tf.keras.utils.to_categorical(y_batch_train,num_classes=self.no_classes), y_pred)\n",
        "          metrics.update({'train_acc':self.train_acc_metric.result()})\n",
        "          pbar.update(step+1, values=metrics.items(), finalize=False)\n",
        "\n",
        "\n",
        "      # Run a validation loop at the end of each epoch.\n",
        "      for x_batch_val, y_batch_val in validation_dataset:\n",
        "        y_batch_val=tf.keras.utils.to_categorical(y_batch_val,num_classes=self.no_classes)\n",
        "        val_pred = self(x_batch_val,y_batch_val,training=False) # $ better design needed\n",
        "        # Update val metrics\n",
        "        self.val_acc_metric.update_state(y_batch_val, val_pred)\n",
        "\n",
        "      metrics.update({'val_acc':self.val_acc_metric.result()})\n",
        "      \n",
        "      pbar.update(step+1, values=metrics.items(), finalize=True)\n",
        "      \n",
        "      # Reset training & val metrics at the end of each epoch\n",
        "      self.train_acc_metric.reset_states()\n",
        "      self.val_acc_metric.reset_states()\n"
      ],
      "metadata": {
        "id": "JhkpEVMiBGBv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls drive/MyDrive/CXRDATA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2PXFtSHJZWM",
        "outputId": "b0349f57-9b8f-4ddc-8948-31a21c8d8fae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mTest\u001b[0m/  \u001b[01;34mTrain\u001b[0m/  Train.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH=\"drive/MyDrive/CXRDATA\"\n",
        "\n",
        "train_dir = PATH+\"/Train\" \n",
        "validation_dir = PATH+\"/Test\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (256, 256)\n",
        "\n",
        "#train data\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
        "                                                            shuffle=True,\n",
        "                                                            batch_size=BATCH_SIZE,\n",
        "                                                            image_size=IMG_SIZE)\n",
        "\n",
        "print('Number of validation batches: %d' % tf.data.experimental.cardinality(train_dataset))\n",
        "\n",
        "#validation model.\n",
        "validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
        "                                                                 shuffle=True,\n",
        "                                                                 batch_size=BATCH_SIZE,\n",
        "                                                                 image_size=IMG_SIZE)\n",
        "\n",
        "\n",
        "# creating test data.\n",
        "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
        "test_dataset = validation_dataset.take(val_batches // 5)\n",
        "validation_dataset = validation_dataset.skip(val_batches // 5)\n",
        "\n",
        "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
        "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxgMXTltALnP",
        "outputId": "8fc0706b-473f-492f-89c0-653e85a05514"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12121 files belonging to 3 classes.\n",
            "Number of validation batches: 379\n",
            "Found 3032 files belonging to 3 classes.\n",
            "Number of validation batches: 76\n",
            "Number of test batches: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optimization parameter setting.\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "Q_-kumszAZzt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKp_pwF4KrTH",
        "outputId": "39d015a8-233e-487d-b493-27051f8af59a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Caps_net(no_classes=3)"
      ],
      "metadata": {
        "id": "YaBhYK8RAeR0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset,validation_dataset,epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OsQZ3KBCH5qO",
        "outputId": "ded455ce-b349-48aa-ec30-bc22abecc4a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch 1/30\n",
            "379/379 [==============================] - 1775s 5s/step - train_acc: 0.6605 - val_acc: 0.6679\n",
            "\n",
            "epoch 2/30\n",
            "379/379 [==============================] - 137s 360ms/step - train_acc: 0.6767 - val_acc: 0.6700\n",
            "\n",
            "epoch 3/30\n",
            "379/379 [==============================] - 151s 398ms/step - train_acc: 0.6749 - val_acc: 0.6716\n",
            "\n",
            "epoch 4/30\n",
            "379/379 [==============================] - 137s 360ms/step - train_acc: 0.6743 - val_acc: 0.6712\n",
            "\n",
            "epoch 5/30\n",
            "379/379 [==============================] - 151s 398ms/step - train_acc: 0.6749 - val_acc: 0.6733\n",
            "\n",
            "epoch 6/30\n",
            "379/379 [==============================] - 136s 358ms/step - train_acc: 0.6746 - val_acc: 0.6667\n",
            "\n",
            "epoch 7/30\n",
            "379/379 [==============================] - 136s 356ms/step - train_acc: 0.6763 - val_acc: 0.6712\n",
            "\n",
            "epoch 8/30\n",
            "379/379 [==============================] - 136s 357ms/step - train_acc: 0.6769 - val_acc: 0.6749\n",
            "\n",
            "epoch 9/30\n",
            "379/379 [==============================] - 151s 398ms/step - train_acc: 0.6770 - val_acc: 0.6679\n",
            "\n",
            "epoch 10/30\n",
            "379/379 [==============================] - 151s 398ms/step - train_acc: 0.6750 - val_acc: 0.6724\n",
            "\n",
            "epoch 11/30\n",
            "379/379 [==============================] - 136s 358ms/step - train_acc: 0.6758 - val_acc: 0.6708\n",
            "\n",
            "epoch 12/30\n",
            "379/379 [==============================] - 151s 397ms/step - train_acc: 0.6760 - val_acc: 0.6700\n",
            "\n",
            "epoch 13/30\n",
            "379/379 [==============================] - 151s 398ms/step - train_acc: 0.6761 - val_acc: 0.6762\n",
            "\n",
            "epoch 14/30\n",
            "379/379 [==============================] - 152s 400ms/step - train_acc: 0.6773 - val_acc: 0.6683\n",
            "\n",
            "epoch 15/30\n",
            "379/379 [==============================] - 151s 398ms/step - train_acc: 0.6759 - val_acc: 0.6745\n",
            "\n",
            "epoch 16/30\n",
            "379/379 [==============================] - 151s 398ms/step - train_acc: 0.6773 - val_acc: 0.6700\n",
            "\n",
            "epoch 17/30\n",
            "379/379 [==============================] - 151s 398ms/step - train_acc: 0.6758 - val_acc: 0.6745\n",
            "\n",
            "epoch 18/30\n",
            "379/379 [==============================] - 152s 398ms/step - train_acc: 0.6764 - val_acc: 0.6679\n",
            "\n",
            "epoch 19/30\n",
            "379/379 [==============================] - ETA: 0s - train_acc: 0.6750"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9dfcd4dcb458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-7ddaf2ed2726>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataset, validation_dataset, epochs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0;31m# Iterate over the batches of the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m               \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# $ better design needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('./checkpoints/my_checkpoint')"
      ],
      "metadata": {
        "id": "winEiAD8K6bb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yxl7o8UIdLTY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}